{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = pd.read_csv('dalas/project/githubProject/ConflictPrediction/dataset2/dataset2_before_splitting.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## On repasse le type des colonnes 'was_interpolated' en int au lieu de bool\n",
    "\n",
    "cols_to_convert = [\n",
    "    c for c in dataset2.columns \n",
    "    if \"_was_interpolated\" in c\n",
    "]\n",
    "\n",
    "for col in cols_to_convert:\n",
    "    dataset2[col] = dataset2[col].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset2 = dataset2.drop(columns=['Unnamed: 0'])\n",
    "#dataset2.to_csv('dalas/project/githubProject/ConflictPrediction/dataset2/dataset2_before_splitting.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (6610, 166)\n",
      "Test shape: (735, 166)\n"
     ]
    }
   ],
   "source": [
    "## Code pour split le dataset 1 en train/test 90-10 %\n",
    "\n",
    "X = dataset2.drop(columns=['war'])\n",
    "y = dataset2['war']\n",
    "\n",
    "# Split train/test final\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,y,\n",
    "    test_size=0.1,# 10% pour test\n",
    "    stratify=y, #pour garder la répartition des classes\n",
    "    random_state=42 #pour mettre une random seed fixe\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "MLP Metrics:\n",
      "Accuracy : 0.8903\n",
      "Precision: 0.8660\n",
      "Recall   : 0.8612\n",
      "F1-score : 0.8636\n",
      "Fold 1:\n",
      "MLP Metrics:\n",
      "Accuracy : 0.9017\n",
      "Precision: 0.8711\n",
      "Recall   : 0.8874\n",
      "F1-score : 0.8792\n",
      "Fold 2:\n",
      "MLP Metrics:\n",
      "Accuracy : 0.9274\n",
      "Precision: 0.9024\n",
      "Recall   : 0.9193\n",
      "F1-score : 0.9108\n",
      "Fold 3:\n",
      "MLP Metrics:\n",
      "Accuracy : 0.9032\n",
      "Precision: 0.8872\n",
      "Recall   : 0.8705\n",
      "F1-score : 0.8788\n",
      "Fold 4:\n",
      "MLP Metrics:\n",
      "Accuracy : 0.8464\n",
      "Precision: 0.8300\n",
      "Recall   : 0.7786\n",
      "F1-score : 0.8035\n",
      "Average F1-score MLP over 5 folds: 0.8671658604473531\n",
      "Standard Deviation of F1-score across all folds for MLP: 0.035354569257847926\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "### Traitement avec un MLPClassifier \n",
    "\n",
    "mlp = MLPClassifier(# paramètres du modèle à ajuster\n",
    "    hidden_layer_sizes=(128, 64),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=200,\n",
    "    random_state=42\n",
    ")\n",
    "F1ScoreListMLP = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for i,(train_index,val_index) in enumerate(skf.split(X_train, y_train)):\n",
    "\n",
    "    # On récupère les sous-datasets d'entraînement et de validation\n",
    "    X_tr = X_train.iloc[train_index]\n",
    "    X_val=  X_train.iloc[val_index]\n",
    "    y_tr  = y_train.iloc[train_index] \n",
    "    y_val = y_train.iloc[val_index]\n",
    "\n",
    "    # Entraînement du modèle MLP\n",
    "    mlp.fit(X_tr, y_tr)\n",
    "    y_pred = mlp.predict(X_val)\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Fold {i}:\")\n",
    "\n",
    "    # Métriques MLP\n",
    "    accMLP = accuracy_score(y_val, y_pred)\n",
    "    precMLP = precision_score(y_val, y_pred)\n",
    "    recMLP = recall_score(y_val, y_pred)\n",
    "    f1MLP = f1_score(y_val, y_pred)\n",
    "\n",
    "    \n",
    "    F1ScoreListMLP.append(f1MLP)\n",
    "    \n",
    "\n",
    "    print(\"MLP Metrics:\")\n",
    "    print(f\"Accuracy : {accMLP:.4f}\")\n",
    "    print(f\"Precision: {precMLP:.4f}\")\n",
    "    print(f\"Recall   : {recMLP:.4f}\")\n",
    "    print(f\"F1-score : {f1MLP:.4f}\")\n",
    "\n",
    "print(\"Average F1-score MLP over 5 folds:\", np.mean(F1ScoreListMLP))\n",
    "print(\"Standard Deviation of F1-score across all folds for MLP: \" + str(np.std(F1ScoreListMLP)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKF avec K =10 (on garde le même skf pour avoir la meme répartition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf_10 = StratifiedKFold(n_splits=10, shuffle=True, random_state=42) # On garde le même skf pour la comparaison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "XGBoost Metrics:\n",
      "F1-score : 0.8885\n",
      "Fold 1:\n",
      "XGBoost Metrics:\n",
      "F1-score : 0.8906\n",
      "Fold 2:\n",
      "XGBoost Metrics:\n",
      "F1-score : 0.8854\n",
      "Fold 3:\n",
      "XGBoost Metrics:\n",
      "F1-score : 0.8794\n",
      "Fold 4:\n",
      "XGBoost Metrics:\n",
      "F1-score : 0.9052\n",
      "Fold 5:\n",
      "XGBoost Metrics:\n",
      "F1-score : 0.8945\n",
      "Fold 6:\n",
      "XGBoost Metrics:\n",
      "F1-score : 0.9014\n",
      "Fold 7:\n",
      "XGBoost Metrics:\n",
      "F1-score : 0.8919\n",
      "Fold 8:\n",
      "XGBoost Metrics:\n",
      "F1-score : 0.8606\n",
      "Fold 9:\n",
      "XGBoost Metrics:\n",
      "F1-score : 0.8915\n",
      "\n",
      "Average F1-score across all folds for XGBoost: 0.8889030012666824\n",
      "Standard Deviation of F1-score across all folds for XGBoost: 0.011740895149880314\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "## Modèle XGBoost\n",
    "\n",
    "F1ScoreListXGB = []\n",
    "\n",
    "\n",
    "for i,(train_index,val_index) in enumerate(skf_10.split(X_train, y_train)):\n",
    "\n",
    "    # On récupère les sous-datasets d'entraînement et de validation\n",
    "    X_tr = X_train.iloc[train_index]\n",
    "    X_val=  X_train.iloc[val_index]\n",
    "    y_tr  = y_train.iloc[train_index] \n",
    "    y_val = y_train.iloc[val_index]\n",
    "\n",
    "    # Entrainement du modèle XGBoost\n",
    "     \n",
    "    modelXgb = xgb.XGBClassifier(# Hyperparamètres à revoir\n",
    "            n_estimators=100,\n",
    "            max_depth=5,\n",
    "            learning_rate=1,\n",
    "            random_state=42\n",
    "        )\n",
    "    # Entraînement\n",
    "    modelXgb.fit(X_tr, y_tr)\n",
    "    y_predXgb = modelXgb.predict(X_val)\n",
    "    print(f\"Fold {i}:\")\n",
    "\n",
    "    # Métriques XGBoost\n",
    "    accXGB = accuracy_score(y_val, y_predXgb)\n",
    "    precXGB = precision_score(y_val, y_predXgb)\n",
    "    recXGB = recall_score(y_val, y_predXgb)\n",
    "    f1XGB = f1_score(y_val, y_predXgb)\n",
    "\n",
    "    F1ScoreListXGB.append(f1XGB)\n",
    "\n",
    "    print(\"XGBoost Metrics:\")\n",
    "    #print(f\"Accuracy : {accXGB:.4f}\")\n",
    "    #print(f\"Precision: {precXGB:.4f}\")\n",
    "    #print(f\"Recall   : {recXGB:.4f}\")\n",
    "    print(f\"F1-score : {f1XGB:.4f}\")\n",
    "\n",
    "print(\"\\nAverage F1-score across all folds for XGBoost: \" + str(np.mean(F1ScoreListXGB)))\n",
    "print (\"Standard Deviation of F1-score across all folds for XGBoost: \" + str(np.std(F1ScoreListXGB)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "CatBoost Metrics:\n",
      "F1-score : 0.8829\n",
      "Fold 1:\n",
      "CatBoost Metrics:\n",
      "F1-score : 0.9125\n",
      "Fold 2:\n",
      "CatBoost Metrics:\n",
      "F1-score : 0.9231\n",
      "Fold 3:\n",
      "CatBoost Metrics:\n",
      "F1-score : 0.9087\n",
      "Fold 4:\n",
      "CatBoost Metrics:\n",
      "F1-score : 0.9056\n",
      "Fold 5:\n",
      "CatBoost Metrics:\n",
      "F1-score : 0.9052\n",
      "Fold 6:\n",
      "CatBoost Metrics:\n",
      "F1-score : 0.9062\n",
      "Fold 7:\n",
      "CatBoost Metrics:\n",
      "F1-score : 0.9049\n",
      "Fold 8:\n",
      "CatBoost Metrics:\n",
      "F1-score : 0.8713\n",
      "Fold 9:\n",
      "CatBoost Metrics:\n",
      "F1-score : 0.8941\n",
      "\n",
      "Average F1-score CatBoost: 0.9014599032516966\n",
      "Std CatBoost: 0.014202074634989919\n"
     ]
    }
   ],
   "source": [
    "## Modèle CatBoost\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "F1ScoreListCB = []\n",
    "\n",
    "\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(skf_10.split(X_train, y_train)):\n",
    "\n",
    "    X_tr = X_train.iloc[train_index]\n",
    "    X_val = X_train.iloc[val_index]\n",
    "    y_tr = y_train.iloc[train_index]\n",
    "    y_val = y_train.iloc[val_index]\n",
    "\n",
    "    # CatBoostClassifier\n",
    "    modelCB = CatBoostClassifier( # A revoir\n",
    "        iterations=300,        \n",
    "        learning_rate=0.05,\n",
    "        depth=6,\n",
    "        loss_function='Logloss',\n",
    "        verbose=False,         \n",
    "        random_seed=42\n",
    "    )\n",
    "\n",
    "    modelCB.fit(X_tr, y_tr)\n",
    "\n",
    "    y_predCB = modelCB.predict(X_val)\n",
    "\n",
    "    print(f\"Fold {i}:\")\n",
    "    accCB = accuracy_score(y_val, y_predCB)\n",
    "    precCB = precision_score(y_val, y_predCB)\n",
    "    recCB = recall_score(y_val, y_predCB)\n",
    "    f1CB = f1_score(y_val, y_predCB)\n",
    "\n",
    "    F1ScoreListCB.append(f1CB)\n",
    "\n",
    "    print(\"CatBoost Metrics:\")\n",
    "    #print(f\"Accuracy : {accCB:.4f}\")\n",
    "    #print(f\"Precision: {precCB:.4f}\")\n",
    "    #print(f\"Recall   : {recCB:.4f}\")\n",
    "    print(f\"F1-score : {f1CB:.4f}\")\n",
    "\n",
    "print(\"\\nAverage F1-score CatBoost:\", np.mean(F1ScoreListCB))\n",
    "print(\"Std CatBoost:\", np.std(F1ScoreListCB))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "GradientBoosting Metrics:\n",
      "F1-score : 0.6908\n",
      "Fold 1:\n",
      "GradientBoosting Metrics:\n",
      "F1-score : 0.6907\n",
      "Fold 2:\n",
      "GradientBoosting Metrics:\n",
      "F1-score : 0.6637\n",
      "Fold 3:\n",
      "GradientBoosting Metrics:\n",
      "F1-score : 0.6848\n",
      "Fold 4:\n",
      "GradientBoosting Metrics:\n",
      "F1-score : 0.7035\n"
     ]
    }
   ],
   "source": [
    "#GradientBoostingClassifier\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "F1ScoreListGB = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(skf_10.split(X_train, y_train)):\n",
    "\n",
    "    X_tr = X_train.iloc[train_index]\n",
    "    X_val = X_train.iloc[val_index]\n",
    "    y_tr = y_train.iloc[train_index]\n",
    "    y_val = y_train.iloc[val_index]\n",
    "\n",
    "    modelGB = GradientBoostingClassifier(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=3\n",
    "    )\n",
    "\n",
    "    modelGB.fit(X_tr, y_tr)\n",
    "    y_predGB = modelGB.predict(X_val)\n",
    "\n",
    "    print(f\"Fold {i}:\")\n",
    "    accGB = accuracy_score(y_val, y_predGB)\n",
    "    precGB = precision_score(y_val, y_predGB)\n",
    "    recGB = recall_score(y_val, y_predGB)\n",
    "    f1GB = f1_score(y_val, y_predGB)\n",
    "\n",
    "    F1ScoreListGB.append(f1GB)\n",
    "\n",
    "    print(\"GradientBoosting Metrics:\")\n",
    "    #print(f\"Accuracy : {accGB:.4f}\")\n",
    "    #print(f\"Precision: {precGB:.4f}\")\n",
    "    #print(f\"Recall   : {recGB:.4f}\")\n",
    "    print(f\"F1-score : {f1GB:.4f}\")\n",
    "\n",
    "print(\"\\nAverage F1-score GB:\", np.mean(F1ScoreListGB))\n",
    "print(\"Std GB:\", np.std(F1ScoreListGB))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "ExtraTrees Metrics:\n",
      "F1-score : 0.9662\n",
      "Fold 1:\n",
      "ExtraTrees Metrics:\n",
      "F1-score : 0.9509\n",
      "Fold 2:\n",
      "ExtraTrees Metrics:\n",
      "F1-score : 0.9719\n",
      "Fold 3:\n",
      "ExtraTrees Metrics:\n",
      "F1-score : 0.9513\n",
      "Fold 4:\n",
      "ExtraTrees Metrics:\n",
      "F1-score : 0.9645\n",
      "Fold 5:\n",
      "ExtraTrees Metrics:\n",
      "F1-score : 0.9695\n",
      "Fold 6:\n",
      "ExtraTrees Metrics:\n",
      "F1-score : 0.9679\n",
      "Fold 7:\n",
      "ExtraTrees Metrics:\n",
      "F1-score : 0.9621\n",
      "Fold 8:\n",
      "ExtraTrees Metrics:\n",
      "F1-score : 0.9432\n",
      "Fold 9:\n",
      "ExtraTrees Metrics:\n",
      "F1-score : 0.9598\n",
      "\n",
      "Average F1-score ET: 0.9607242673000433\n",
      "Std ET: 0.008887213470866927\n"
     ]
    }
   ],
   "source": [
    "#ExtraTreesClassifier\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "F1ScoreListET = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(skf_10.split(X_train, y_train)):\n",
    "\n",
    "    X_tr = X_train.iloc[train_index]\n",
    "    X_val = X_train.iloc[val_index]\n",
    "    y_tr = y_train.iloc[train_index]\n",
    "    y_val = y_train.iloc[val_index]\n",
    "\n",
    "    modelET = ExtraTreesClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    modelET.fit(X_tr, y_tr)\n",
    "    y_predET = modelET.predict(X_val)\n",
    "\n",
    "    print(f\"Fold {i}:\")\n",
    "    accET = accuracy_score(y_val, y_predET)\n",
    "    precET = precision_score(y_val, y_predET)\n",
    "    recET = recall_score(y_val, y_predET)\n",
    "    f1ET = f1_score(y_val, y_predET)\n",
    "\n",
    "    F1ScoreListET.append(f1ET)\n",
    "\n",
    "    print(\"ExtraTrees Metrics:\")\n",
    "    #print(f\"Accuracy : {accET:.4f}\")\n",
    "    #print(f\"Precision: {precET:.4f}\")\n",
    "    #print(f\"Recall   : {recET:.4f}\")\n",
    "    print(f\"F1-score : {f1ET:.4f}\")\n",
    "\n",
    "print(\"\\nAverage F1-score ET:\", np.mean(F1ScoreListET))\n",
    "print(\"Std ET:\", np.std(F1ScoreListET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "RandomForest Metrics:\n",
      "F1-score : 0.9492\n",
      "Fold 1:\n",
      "RandomForest Metrics:\n",
      "F1-score : 0.9488\n",
      "Fold 2:\n",
      "RandomForest Metrics:\n",
      "F1-score : 0.9529\n",
      "Fold 3:\n",
      "RandomForest Metrics:\n",
      "F1-score : 0.9459\n",
      "Fold 4:\n",
      "RandomForest Metrics:\n",
      "F1-score : 0.9468\n",
      "Fold 5:\n",
      "RandomForest Metrics:\n",
      "F1-score : 0.9577\n",
      "Fold 6:\n",
      "RandomForest Metrics:\n",
      "F1-score : 0.9600\n",
      "Fold 7:\n",
      "RandomForest Metrics:\n",
      "F1-score : 0.9464\n",
      "Fold 8:\n",
      "RandomForest Metrics:\n",
      "F1-score : 0.9207\n",
      "Fold 9:\n",
      "RandomForest Metrics:\n",
      "F1-score : 0.9480\n",
      "\n",
      "Average F1-score RF: 0.9476277837799894\n",
      "Std RF: 0.010082585452266746\n"
     ]
    }
   ],
   "source": [
    "#RandomForestClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "F1ScoreListRF = []\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(skf_10.split(X_train, y_train)):\n",
    "\n",
    "    X_tr = X_train.iloc[train_index]\n",
    "    X_val = X_train.iloc[val_index]\n",
    "    y_tr = y_train.iloc[train_index]\n",
    "    y_val = y_train.iloc[val_index]\n",
    "\n",
    "    modelRF = RandomForestClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=None,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    modelRF.fit(X_tr, y_tr)\n",
    "    y_predRF = modelRF.predict(X_val)\n",
    "\n",
    "    print(f\"Fold {i}:\")\n",
    "    accRF = accuracy_score(y_val, y_predRF)\n",
    "    precRF = precision_score(y_val, y_predRF)\n",
    "    recRF = recall_score(y_val, y_predRF)\n",
    "    f1RF = f1_score(y_val, y_predRF)\n",
    "\n",
    "    F1ScoreListRF.append(f1RF)\n",
    "\n",
    "    print(\"RandomForest Metrics:\")\n",
    "    #print(f\"Accuracy : {accRF:.4f}\")\n",
    "    #print(f\"Precision: {precRF:.4f}\")\n",
    "    #print(f\"Recall   : {recRF:.4f}\")\n",
    "    print(f\"F1-score : {f1RF:.4f}\")\n",
    "\n",
    "print(\"\\nAverage F1-score RF:\", np.mean(F1ScoreListRF))\n",
    "print(\"Std RF:\", np.std(F1ScoreListRF))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TP Notebook",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
